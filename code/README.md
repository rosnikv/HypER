## HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance

This directory includes:
- Scripts to construct temporally ordered reasoning chains from citation graphs
- Expert-validated examples `llama_consistency_experts/` and relevance data for LLaMA-based consistency evaluation
- Configuration and scripts for LoRA-based multi-task fine-tuning
- Inference pipeline for hypothesis generation
- Example data outputs evaluated by domain experts `expert_eval_final(RQ2)/` for qualitative analysis
---

## Directory Structure

### `chain_construction/`
Contains scripts to construct valid and invalid reasoning chains by traversing citation graphs and scoring papers via LLM.

#### Contents
- `invalid_chains.py`: Generates easy negative chains by randomly swapping intermediate nodes with irrelevant ones.
- `invalid_chains_hard.py`: Generates hard negative chains with coherent subchains but hidden logical breaks.
- `RAG_temporal_extension_llama.py`: Generates valid reasoning chain.
- `review_ids.txt`: List of review IDs used to seed chain construction.
- `run_with_seeds.sh`: End-to-end pipeline to generate valid reasoning chains from selected reviews in `review_ids.txt`.
- `utils.py`: Utility functions for filtering, scoring, and chain formatting.
- `prompts/` : Contains all LLM prompt templates used for relevance scoring between papers

##### Expert validated samples and consistency analysis with llama
Contains scripts and annotated results used to evaluate the **LLM-based consistency scoring**, with comparisons to human expert judgment.

- `llama_consistency_experts/` : Expert-validated scoring files for 3 annotators
- `consistency_analysis.ipynb`: Script to compare LLM vs. expert agreement (e.g., Cohen's Kappa).

## Example Usage (Temporal Chain Construction)

Add the required PMIDS in the `review_ids.txt` and run:
```bash
# Step: Generate reasoning chains
bash chain_construction/run_with_seeds.sh
```

---

### `dataset_code/`
Scripts and utilities for preparing the reasoning chains and multi-task data for model training.

#### Contents
- `balanced_splits_w_hyp/`: Raw train/val/test splits of chains labeled with reasoning validity.
- `multi-task-data_prep.py`: Script to convert balanced chain data into multi-task format for training.
- `collect_paths.ipynb`: Extracts reasoning paths and creates chain-level summaries, also script for converting to alpaca formatted dataset.
- `groundtruth_statistics.ipynb`: Computes task and label distributions for analysis and construct the balanced dataset.

---

### `fine-tuning/`
We use the [Axolotl framework](https://github.com/OpenAccess-AI-Collective/axolotl) with `accelerate` for scalable multi-GPU fine-tuning. Below are working configurations used for training and inference.

## Example Usage
```bash
# Step: Fine-tune model
accelerate launch -m axolotl.cli.train axolotl/phi3-lora-HypER_multihop12.yaml
CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7,8" python3 -m axolotl.cli.merge_lora examples/phi/phi3-lora-HypER_multihop12.yaml --lora_model_dir="./outputs/phi3-hypER-mixed-lora-out-full"
```
> **Note**: The final fine-tuned HypER model (full-model) will be released on Hugging Face for reproducibility and downstream use.

---

### `hypothesis_generation/`
This folder contains scripts for generating hypotheses from reasoning chains using different models (e.g., HypER, GPT-4, baseline), as well as evaluating them using judge model.

- `phi_hypothesis_gen.py`: Main script for generating hypotheses using the fine-tuned HypER (Phi-3) model.
- `researchagent.py`: Adapted prompt from (Baek et al., 2024) for hypotheses generation.
- `novelty_rag_001.py`: Script to compute novelty using retrieval-augmented generation against prior literature.
- `fewshots_rating.json`: Contains few-shot examples for LLM-based evaluators.
- `scoring_protocol.json`: Defines the LLM-as-judge scoring rubric (clarity, feasibility, impact, etc.) borrowed from from (Baek et al., 2024).

## Example Usage
```bash
# Step: Generate hypothesis (inference)
python phi_hypothesis_gen.py
```

---

### `hg_evaluation/`
This folder contains scripts, annotated outputs, and expert-validated data for evaluating hypotheses generated by the HypER model. It supports both automatic and human expert evaluation pipelines for assessing the hypotheses.

#### Contents
- `expert_eval_final(RQ2)/`: Contains expert-reviewed judgments (RQ2) on a subset of generated hypotheses.
- `Expert_evaluation_summary.xlsx`: Aggregated ratings from 10 domain experts, covering dimensions such as clarity, originality, feasibility, and impact.
- `agreement.ipynb`: Analysis of inter-rater agreement between experts and LLM-as-judge, including metrics like Cohenâ€™s Kappa and rank correlation.
- `final_eval.ipynb`, `gpt4_eval.ipynb`, `eval_invalid.ipynb`: Scripts to analyze the evaluation results, compare generated outputs, and inspect outputs on valid vs. invalid chains.

