{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1b9f80accd444d858c86dbed5712c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1839 [00:00<?, ?it/s]/home/user/rosni/miniconda2/envs/scigen/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  1%|          | 10/1839 [02:26<7:53:25, 15.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 1839/1839 [5:28:52<00:00, 10.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to ./evaluation/mistral-hyper-full3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"../../ground_truth_dataset/multi_task_data3/test_hyp.json\"\n",
    "dataset_path = \"../../ground_truth_dataset/mistral_format_multi_task3/mistral_test.jsonl\"\n",
    "results_output_path = \"./evaluation/mistral-hyper-full3.json\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "base_model_path = \"amazon/MistralLite\"\n",
    "fine_tuned_model_path = \"./outputs/mistralLite-hypER-mixed-lora-out-full2/merged/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    fine_tuned_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Load and parse JSONL dataset\n",
    "def load_and_parse_jsonl(file_path):\n",
    "    parsed_data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line.strip())\n",
    "            parsed_data.append(parse_jsonl_entry(entry))\n",
    "    return parsed_data\n",
    "\n",
    "# Parse individual JSONL entry\n",
    "def parse_jsonl_entry(entry):\n",
    "    \"\"\"\n",
    "    Parse a single JSONL entry into input text and the actual label.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing the \"text\" field.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'input_text' (model input) and 'actual_label' (ground truth).\n",
    "    \"\"\"\n",
    "    full_text = entry[\"text\"]\n",
    "    \n",
    "    # Extract <|assistant|> response\n",
    "    if \"<|assistant|>\" in full_text:\n",
    "        input_text, actual_label = full_text.split(\"<|assistant|>\", 1)\n",
    "        input_text = input_text.strip() + \"<|assistant|>\"  # Ensure model prompt ends correctly\n",
    "        actual_label = actual_label.strip().lower()  # Normalize actual label\n",
    "        return {\n",
    "            \"input_text\": input_text,\n",
    "            \"actual_label\": actual_label\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Entry does not contain <|assistant|> tag.\")\n",
    "\n",
    "# Load and filter metadata\n",
    "def load_metadata(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    \"\"\"Filter out entries where source or target abstract is missing.\"\"\"\n",
    "    filtered_metadata = []\n",
    "\n",
    "    for entry in metadata:\n",
    "        task_name = entry.get(\"task_name\", \"unknown\")\n",
    "        input_data = entry.get(\"input\", {})\n",
    "\n",
    "        if task_name == \"hyper-1-hop\":\n",
    "            source_paper = input_data.get(\"source_paper\", {})\n",
    "            target_paper = input_data.get(\"target_paper\", {})\n",
    "\n",
    "            # Skip if abstracts are missing\n",
    "            if not source_paper.get(\"abstract\") or not target_paper.get(\"abstract\"):\n",
    "                continue  \n",
    "\n",
    "        filtered_metadata.append(entry)\n",
    "\n",
    "    return filtered_metadata\n",
    "\n",
    "# Save predictions\n",
    "def save_predictions(predictions, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Saved predictions to {file_path}\")\n",
    "\n",
    "def evaluate_model_on_dataset(mistral_dataset, metadata, output_path):\n",
    "    predictions = []\n",
    "\n",
    "    # Ensure dataset and metadata are aligned\n",
    "    assert len(mistral_dataset) == len(metadata), f\"Mismatch in dataset and metadata length! ({len(mistral_dataset)} vs {len(metadata)})\"\n",
    "\n",
    "    print(\"Starting inference...\")\n",
    "    for index, entry in tqdm(enumerate(mistral_dataset), total=len(mistral_dataset)):\n",
    "        # Get corresponding metadata\n",
    "        task_metadata = metadata[index]\n",
    "\n",
    "        input_text = entry[\"input_text\"]\n",
    "        actual_label = entry[\"actual_label\"]\n",
    "\n",
    "        # Extract metadata fields\n",
    "        task_name = task_metadata.get(\"task_name\", \"unknown\")\n",
    "        file_name = task_metadata.get(\"file_name\", \"unknown\")\n",
    "        chain_label = task_metadata.get(\"chain_label\", \"unknown\")\n",
    "        file_path = task_metadata.get(\"file_path\", \"unknown\")\n",
    "\n",
    "        try:\n",
    "            # Directly pass input_text without formatting as a message\n",
    "            output = pipe(input_text, **generation_args)\n",
    "            predicted_output = output[0]['generated_text'].strip()\n",
    "\n",
    "            if not predicted_output:\n",
    "                predicted_output = \"Generation failed\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error with entry: {entry}\\nError: {e}\")\n",
    "            predicted_output = \"Generation failed\"\n",
    "\n",
    "        # Append metadata and prediction results\n",
    "        predictions.append({\n",
    "            \"task_name\": task_name,\n",
    "            \"file_name\": file_name,\n",
    "            \"chain_label\": chain_label,\n",
    "            \"file_path\": file_path,\n",
    "            \"input\": input_text,\n",
    "            \"actual_output\": actual_label,\n",
    "            \"predicted_output\": predicted_output\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "mistral_dataset = load_and_parse_jsonl(dataset_path)\n",
    "metadata = load_metadata(metadata_path)\n",
    "filtered_metadata = filter_metadata(metadata)\n",
    "\n",
    "# Run inference and save results\n",
    "evaluate_model_on_dataset(mistral_dataset, filtered_metadata, results_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to saved predictions\n",
    "results_output_path = \"./evaluation/mistral-base.json\"\n",
    "\n",
    "# Load predictions JSON\n",
    "def load_predictions(file_path):\n",
    "    \"\"\"Load saved predictions from JSON.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the saved results\n",
    "predictions = load_predictions(results_output_path)\n",
    "\n",
    "# Initialize storage for task-wise results\n",
    "task_results = defaultdict(lambda: {\"y_true\": [], \"y_pred\": [], \"y_true_invalids\": [], \"y_pred_invalids\": []})\n",
    "\n",
    "# Normalize JSON output (remove formatting issues)\n",
    "import re\n",
    "\n",
    "def extract_validity_and_ids(json_str):\n",
    "    \"\"\"\n",
    "    Extracts 'validity' status and any numeric paper IDs from a JSON string.\n",
    "    Works even if the format is broken.\n",
    "    \"\"\"\n",
    "    if not json_str or json_str.strip() == \"\":\n",
    "        return None, []  # Return None if input is empty\n",
    "\n",
    "    json_str = re.sub(r\"```json|```\", \"\", json_str)  # Removes '```json' and '```'\n",
    "    json_str = re.sub(r\"\\s+\", \" \", json_str).strip() # Remove extra spaces and strip\n",
    "    \n",
    "    # Extract validity status (valid or invalid)\n",
    "    validity_match = re.search(r'\"validity\"\\s*:\\s*\"?(\\bvalid\\b|\\binvalid\\b)\"?', json_str)\n",
    "\n",
    "    validity = validity_match.group(1) if validity_match else None\n",
    "\n",
    "    # Extract numbers from \"invalid_paper_ids\"\n",
    "    invalid_ids = re.findall(r'\\d+', json_str)  # Find all numbers\n",
    "\n",
    "    return validity, list(map(int, invalid_ids))  # Convert numbers to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_relevance_score(json_str, debug=False):\n",
    "    \"\"\"\n",
    "    Extracts the relevance score from a JSON string or integer input.\n",
    "    Handles extra text, multiple JSONs, and unexpected formats.\n",
    "\n",
    "    Args:\n",
    "        json_str (str or int): Input containing relevance score.\n",
    "        debug (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted relevance score or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Directly return if already an integer\n",
    "    if isinstance(json_str, int):\n",
    "        return json_str  \n",
    "\n",
    "    try:\n",
    "        # Ensure input is a string\n",
    "        if not isinstance(json_str, str):\n",
    "            raise TypeError(f\"Expected str or int, got {type(json_str)}\")\n",
    "\n",
    "        # Remove extra formatting (e.g., triple quotes, markdown artifacts)\n",
    "        json_str = re.sub(r\"```json|```\", \"\", json_str).strip()\n",
    "\n",
    "        # Extract JSON-like content even if mixed with extra text\n",
    "        match = re.search(r'\\{[^{}]*\"relevance_score\"[^{}]*\\}', json_str)\n",
    "        if match:\n",
    "            json_str = match.group(0)  # Extract JSON substring\n",
    "\n",
    "        # Parse JSON\n",
    "        parsed_json = json.loads(json_str)\n",
    "\n",
    "        # Directly return if the parsed JSON is an integer\n",
    "        if isinstance(parsed_json, int):\n",
    "            return parsed_json\n",
    "\n",
    "        # Extract \"relevance_score\" if present\n",
    "        if \"relevance_score\" in parsed_json:\n",
    "            return int(parsed_json[\"relevance_score\"])\n",
    "\n",
    "        # Check nested structures\n",
    "        for key, value in parsed_json.items():\n",
    "            if isinstance(value, dict) and \"relevance_score\" in value:\n",
    "                return int(value[\"relevance_score\"])\n",
    "\n",
    "        # If nothing matches, return None (skip from evaluation)\n",
    "        if debug:\n",
    "            print(f\"⚠️ 'relevance_score' not found in JSON: {json_str}\")\n",
    "        return None  \n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:\n",
    "        if debug:\n",
    "            print(f\"Error parsing relevance score from: {json_str}\\nError: {e}\")\n",
    "        return None  # Skip this entry for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'hyper-1-hop',\n",
       " 'file_name': 'temporal_chain_CD005158_p-1.json',\n",
       " 'chain_label': 'valid',\n",
       " 'file_path': '../ground_truth_path/result_chains/temporal_chain_CD005158_p-1.json',\n",
       " 'input': '<|prompter|>Hypotheses are frequently the starting point when undertaking the empirical portion of the scientific process. They state something that the scientific process will attempt to evaluate, corroborate, verify, or falsify. Their purpose is to guide the types of data we collect, analyses we conduct, and inferences we would like to make. You are a scientist. Your task is to evaluate the relevance of a target paper to a source paper in a one-hop citation context. You are given a source paper and a target paper that followed from it. Your job is to determine the degree of relevance between these papers based on scientific progression, logical dependence, and hypothesis inspiration.\\n\\nAssign a relevance score to the target paper based on the following criteria:\\n- 0: No meaningful connection between the target paper and the source paper. This includes review papers (e.g., systematic reviews, meta-analyses) that summarize literature without introducing novel hypotheses or findings.\\n- 1: The key hypothesis in the target paper is inspired by the hypothesis or findings from the source paper.\\n- 2: The key hypothesis in the target paper directly depends on the findings of the source paper. This means the source paper contains sub-hypotheses or foundational results that contribute directly to the target paper\\'s hypothesis.\\n\\nYour task:\\n1. Assign a relevance score (0, 1, or 2) based on the relationship between the source and target paper.\\nAnswer must follow the example output format in JSON:\\n```json\\n{ \"relevance_score\": <0, 1, or 2> }\\n```\\n DO NOT include any additional text, comments, explanations, or formatting variations.\\n ONLY return the JSON object exactly as required.\\nSource Paper:\\nTITLE: Clopidogrel and aspirin versus aspirin alone for the prevention of atherothrombotic events.\\nABSTRACT: BACKGROUND\\nDual antiplatelet therapy with clopidogrel plus low-dose aspirin has not been studied in a broad population of patients at high risk for atherothrombotic events.\\n\\n\\nMETHODS\\nWe randomly assigned 15,603 patients with either clinically evident cardiovascular disease or multiple risk factors to receive clopidogrel (75 mg per day) plus low-dose aspirin (75 to 162 mg per day) or placebo plus low-dose aspirin and followed them for a median of 28 months. The primary efficacy end point was a composite of myocardial infarction, stroke, or death from cardiovascular causes.\\n\\n\\nRESULTS\\nThe rate of the primary efficacy end point was 6.8 percent with clopidogrel plus aspirin and 7.3 percent with placebo plus aspirin (relative risk, 0.93; 95 percent confidence interval, 0.83 to 1.05; P=0.22). The respective rate of the principal secondary efficacy end point, which included hospitalizations for ischemic events, was 16.7 percent and 17.9 percent (relative risk, 0.92; 95 percent confidence interval, 0.86 to 0.995; P=0.04), and the rate of severe bleeding was 1.7 percent and 1.3 percent (relative risk, 1.25; 95 percent confidence interval, 0.97 to 1.61 percent; P=0.09). The rate of the primary end point among patients with multiple risk factors was 6.6 percent with clopidogrel and 5.5 percent with placebo (relative risk, 1.2; 95 percent confidence interval, 0.91 to 1.59; P=0.20) and the rate of death from cardiovascular causes also was higher with clopidogrel (3.9 percent vs. 2.2 percent, P=0.01). In the subgroup with clinically evident atherothrombosis, the rate was 6.9 percent with clopidogrel and 7.9 percent with placebo (relative risk, 0.88; 95 percent confidence interval, 0.77 to 0.998; P=0.046).\\n\\n\\nCONCLUSIONS\\nIn this trial, there was a suggestion of benefit with clopidogrel treatment in patients with symptomatic atherothrombosis and a suggestion of harm in patients with multiple risk factors. Overall, clopidogrel plus aspirin was not significantly more effective than aspirin alone in reducing the rate of myocardial infarction, stroke, or death from cardiovascular causes. (ClinicalTrials.gov number, NCT00050817.).\\n\\nTarget Paper:\\nTITLE: Interindividual variability in the efficacy of oral antiplatelet drugs: definitions, mechanisms and clinical importance.\\nABSTRACT: The modern era of antiplatelet therapy was founded by large clinical trials demonstrating the benefit of aspirin and clopidogrel in the treatment and prevention of acute coronary syndromes. The concept of antiplatelet drug \"resistance\" emerged during the 1990s with studies revealing considerable residual platelet aggregation in some patients despite aspirin treatment. In the wake of these reports, larger studies established an association between high on-treatment platelet reactivity and thrombotic events. The possible mechanisms explaining this phenomenon are manifold and reflect the complexity of platelet function, thrombus formation and cardiovascular disease. Some mechanisms apply to both drugs, while others apply only to one of them. In recent years, efforts have been made to translate this information into an improved clinical outcome by modifying antiplatelet drug regimens. Several studies investigated measurements of on-treatment platelet reactivity, but large clinical trials have failed to demonstrate substantial clinical benefit of individually tailored antiplatelet therapy according to platelet function tests. This article provides an integrated review of interindividual variability in the efficacy of aspirin and clopidogrel with particular emphasis on possible effect-modifying mechanisms and clinical implications.</s><|assistant|>',\n",
       " 'actual_output': '2',\n",
       " 'predicted_output': '1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Prediction 0\n",
      "Task Name: hyper-1-hop\n",
      "Actual Output: 2\n",
      "Predicted Output: 1\n",
      "\n",
      "🔍 Prediction 1\n",
      "Task Name: hyper-multi-hop2_c1\n",
      "Actual Output: ```json{\"validity\": valid,\n",
      "\"invalid_paper_ids\": []}```\n",
      "Predicted Output: 196 healthy Chinese male subjects were enrolled. The in vitro inhibition of platelet aggregation (IPA) was evaluated before and after ticagrelor incubated with platelet rich plasma from 196\n",
      "\n",
      "🔍 Prediction 2\n",
      "Task Name: hyper-multi-hop2_c2\n",
      "Actual Output: ```json{\"validity\": valid,\n",
      "\"invalid_paper_ids\": []}```\n",
      "Predicted Output: The reasoning chain is invalid. The final hypotheses in the last paper do not logically depend on the previous papers.\n",
      "\n",
      "Invalid paper IDs: 3, 4, 7\n",
      "\n",
      "🔍 Prediction 3\n",
      "Task Name: hyper-1-hop\n",
      "Actual Output: 0\n",
      "Predicted Output: 1\n",
      "\n",
      "🔍 Prediction 4\n",
      "Task Name: hyper-multi-hop2_c1\n",
      "Actual Output: ```json{\"validity\": invalid,\n",
      "\"invalid_paper_ids\": [2]}```\n",
      "Predicted Output: 196 healthy Chinese male subjects were enrolled in this study. The in vitro inhibition of platelet aggregation (IPA) was evaluated before and after ticagrelor incubated with platelet rich plasma from\n"
     ]
    }
   ],
   "source": [
    "# Print some sample results\n",
    "for i in range(5):  # Print first 5 results\n",
    "    print(f\"\\nPrediction {i}\")\n",
    "    print(f\"Task Name: {predictions[i]['task_name']}\")\n",
    "    print(f\"Actual Output: {predictions[i]['actual_output']}\")\n",
    "    print(f\"Predicted Output: {predictions[i]['predicted_output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, entry in enumerate(tqdm(predictions, total=len(predictions))):\n",
    "    task_name = entry[\"task_name\"]\n",
    "    actual_output = entry[\"actual_output\"]\n",
    "    predicted_output = entry[\"predicted_output\"]\n",
    "\n",
    "    if task_name == \"hyper-1-hop\":\n",
    "        y_true = extract_relevance_score(actual_output)\n",
    "        y_pred = extract_relevance_score(predicted_output)\n",
    "\n",
    "    elif task_name in {\"hyper-multi-hop2_c1\", \"hyper-multi-hop2_c2\"}:\n",
    "        y_true, y_true_invalids = extract_validity_and_ids(actual_output)\n",
    "        y_pred, y_pred_invalids = extract_validity_and_ids(predicted_output)\n",
    "        \n",
    "        # Store invalid paper IDs\n",
    "        task_results[task_name][\"y_true_invalids\"].append(y_true_invalids)\n",
    "        task_results[task_name][\"y_pred_invalids\"].append(y_pred_invalids)\n",
    "\n",
    "    else:\n",
    "        continue  # Ignore unknown task names\n",
    "\n",
    "    # Skip if extraction failed\n",
    "    if y_true is None or y_pred is None:\n",
    "        print(f\"⚠️ Skipping entry {index} due to parsing issues.\")\n",
    "        continue\n",
    "\n",
    "    # Store results in structured format\n",
    "    task_results[task_name][\"y_true\"].append(y_true)\n",
    "    task_results[task_name][\"y_pred\"].append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Task: hyper-1-hop\n",
      "✅ Accuracy: 70.09%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82       574\n",
      "           1       0.00      0.00      0.00        42\n",
      "           2       0.00      0.00      0.00       203\n",
      "\n",
      "    accuracy                           0.70       819\n",
      "   macro avg       0.23      0.33      0.27       819\n",
      "weighted avg       0.49      0.70      0.58       819\n",
      "\n",
      "\n",
      "🔹 Task: hyper-multi-hop2_c1\n",
      "✅ Accuracy: 68.04%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     invalid       0.61      0.97      0.75       253\n",
      "       valid       0.94      0.39      0.55       257\n",
      "\n",
      "    accuracy                           0.68       510\n",
      "   macro avg       0.77      0.68      0.65       510\n",
      "weighted avg       0.77      0.68      0.65       510\n",
      "\n",
      "\n",
      "🔹 Task: hyper-multi-hop2_c2\n",
      "✅ Accuracy: 78.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     invalid       0.71      0.96      0.82       253\n",
      "       valid       0.95      0.61      0.74       257\n",
      "\n",
      "    accuracy                           0.78       510\n",
      "   macro avg       0.83      0.79      0.78       510\n",
      "weighted avg       0.83      0.78      0.78       510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute Accuracy & Metrics for Each Task\n",
    "for task, results in task_results.items():\n",
    "    y_true = results[\"y_true\"]\n",
    "    y_pred = results[\"y_pred\"]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1_jaccard(y_pred_invalids, y_true_invalids):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, F1-score, and Jaccard similarity for invalid paper IDs.\n",
    "    \"\"\"\n",
    "    precisions, recalls, f1s, jaccards = [], [], [], []\n",
    "    \n",
    "    for pred_set, true_set in zip(y_pred_invalids, y_true_invalids):\n",
    "        pred_set, true_set = set(pred_set), set(true_set)\n",
    "\n",
    "        intersection = len(pred_set & true_set)\n",
    "        union = len(pred_set | true_set)\n",
    "\n",
    "        precision = intersection / len(pred_set) if pred_set else 0\n",
    "        recall = intersection / len(true_set) if true_set else 0\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "        jaccard = intersection / union if union > 0 else 1  # Jaccard similarity\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jaccard)\n",
    "\n",
    "    # Compute mean metrics\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    avg_jaccard = np.mean(jaccards)\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1, avg_jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Task: hyper-multi-hop2_c1 (Invalid Paper ID Matching)\n",
      "✅ Precision: 0.12\n",
      "✅ Recall: 0.06\n",
      "✅ F1 Score: 0.07\n",
      "✅ Jaccard Similarity: 0.26\n",
      "\n",
      "🔹 Task: hyper-multi-hop2_c2 (Invalid Paper ID Matching)\n",
      "✅ Precision: 0.12\n",
      "✅ Recall: 0.06\n",
      "✅ F1 Score: 0.07\n",
      "✅ Jaccard Similarity: 0.36\n"
     ]
    }
   ],
   "source": [
    "for task in [\"hyper-multi-hop2_c1\", \"hyper-multi-hop2_c2\"]:\n",
    "    if task in task_results:\n",
    "        precision, recall, f1, jaccard = precision_recall_f1_jaccard(\n",
    "            task_results[task][\"y_pred_invalids\"], task_results[task][\"y_true_invalids\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTask: {task} (Invalid Paper ID Matching)\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "        print(f\"F1 Score: {f1:.2f}\")\n",
    "        print(f\"Jaccard Similarity: {jaccard:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6159999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall f1-score is the average accuracy from all tasks\n",
    "\n",
    "# Compute overall f1-score\n",
    "np.mean([0.58,0.68, 0.68,0.09,0.08 ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scigen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
